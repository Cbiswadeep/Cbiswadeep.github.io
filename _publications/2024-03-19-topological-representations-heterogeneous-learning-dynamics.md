---
title: "Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks"
collection: publications
permalink: /publication/2024-03-19-topological-representations-heterogeneous-learning-dynamics
excerpt: 'This paper explores the unique topological representations learned by Recurrent Spiking Neural Networks (RSNNs) through unsupervised learning methods like spike-timing dependent plasticity (STDP). Introducing a novel application of Representation Topology Divergence (RTD) to RSNNs, the study reveals how heterogeneous learning dynamics contribute to the development of distinct neural representations, offering new insights into the capabilities of SNNs and their potential in creating more biologically plausible hybrid AI systems.'
date: 2024-03-19
venue: 'arXiv preprint arXiv:2403.12462'
citation: 'Biswadeep Chakraborty, Saibal Mukhopadhyay. (2024). "Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks." arXiv preprint arXiv:2403.12462.'
---

Spiking Neural Networks (SNNs), embodying the cutting edge of neuroscience and artificial intelligence, offer a computational model inspired by the neuronal dynamics of the brain. Despite the significant attention on network representations within deep neural networks, the exploration of SNNs, particularly those governed by unsupervised local learning rules like spike-timing dependent plasticity (STDP), has remained underexplored.

Building upon recent advancements, this work leverages the concept of Representation Topology Divergence (RTD)—originally tailored for feedforward networks—to suit the recurrent nature of RSNNs. Through a creative reformulation of RSNNs into feedforward autoencoder networks with strategic skip connections, we render the RTD computation applicable for recurrent architectures. This methodological innovation allows us to quantitatively measure and compare the distributed representations arising from different learning dynamics within RSNNs.

Our investigation focuses on the effects of heterogeneous STDP on learning dynamics, contrasting these with homogeneous learning rules and conventional surrogate gradient-based supervised learning approaches. The findings underscore the distinctive representational capacities endowed by heterogeneity in synaptic dynamics, propelling forward our understanding of RSNNs.

The implications of this study extend beyond theoretical interests, suggesting avenues for harnessing the unique learning capabilities of RSNNs in designing next-generation artificial intelligence systems that more closely mimic the complexity and efficiency of biological neural networks. Through this exploration, we contribute to the broader dialogue on integrating biologically inspired computational models into the fabric of artificial intelligence research and development.
